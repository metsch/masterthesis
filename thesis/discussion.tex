I have evaluated two distinct pooling strategies on an ingest consisting of 1560 digital objects. The change rate of the dataset is 32\%, meaning that a third of the objects are expected to be corrupted or changed during the storage in the archive. With an individual testing strategy, 1560 writing operations on the blockchain are needed. The exact gas cost, to persist 1560 SHA256 values is $1560 \cdot 42,368 = 66,094,080$. The cost in USD for that much gas is \$13218.81, assuming a gas price of 50 \acrlong{gwei} and an \acrlong{eth} price of 4000, see Equation \ref{eq:tx-cost}. It is important to denote the operation cost in gas, because the gas price is changing in the minute ranging from 30 \acrlong{gwei} up to 60 \acrlong{gwei} and the price of the \acrlong{eth} also fluctuates. With a gas price of 30 \acrlong{gwei}, the operation cost for an ingest of 1560 objects would decrease to \$7931.
Therefore, it is important to be aware of the changing gas prices throughout the day. Gas prices are an indicator on how active the Ethereum network is at the moment, e.g. the gas prices are the lowest in the early morning in Europe when North America is mostly asleep.
I have shown, that the amount of costly writing operations can be decreased up to three times utilizing pooled testing. With a two-stage-hierarchical pooling strategy, the amount of writing operations is 520, which results in a total gas cost of $22,031,360$ (\$4406.27).
Although the operation cost was decreased to a third, the cost is still high for a digital archive considering that only a small part of the preservation process is done, persisting fixity information. An operator has to decide what is more important to them, the cost, the security or the convenience of their chosen storage-medium for fixity information. 

My proposed implementation of pooled testing, utilizing hash-lists, does not have any method to find out which exact digital object in the pool has changed. On retrieval of a changed object, one has to get the SHA256 value from the blockchain and re-compute the SHA256 value of a pool with its respective digital objects. If a mismatch occurs of the two cryptographic hashes, there is no way of defining which exact object in the pool has changed. This is due to the fact, that the information on the integrity of single objects is lost while computing the root hash of a hash-list. Only the resulting root hash matters in order to guarantee the integrity of the whole pool. The downside of this method is, that one has to replace the whole pool with correct copies of the objects every time a corrupt pool is detected. In order to lower the amount of these data-scrubbing operations, smaller pool sizes are considered in the proposed pooled testing strategies.
In my thesis, I considered bit level error and data manipulation as possible sources of data change. For legitimate updates, one has to either persist an old version of the object or only persist the SHA256 of the object in order to be able to re-compute the root hash of the original pool. The updated object than have to be pooled in a new pool with other objects or individually and its hash re-uploaded to the Ethereum blockchain.

The first research questions was to find the optimal pool size for the fixity information with respect to two metrics: the first was to minimize the cost and the second was to maximize the efficiency. The two of them were inversely correlated, where to minimize the cost the pool size was actually N, since there was only one writing transaction on the blockchain when we have only one pool on ingest. On the other hand, a pool size of N is extremely inefficient on retrieval. On retrieval, I have to replace the whole ingest bulk with correct copies if only one object in the pool is corrupt, since I cannot determine which object in the pool got corrupted due to the nature of hash lists. Therefore, I had to also account the number of data scrubbing operations into the optimal pool size, resulting in Equation \ref{eq:expected_data-scrubbing}. The pool size is therefore heavily dependent on the change in the ingest bulk, where if the change rate is high smaller pool sizes are favorable.
For the second research question, I have compared two different approaches for pooled testing. Where the efficiency of both strategies is compared to the efficiency of individual testing. The efficiency of individual testing for sample population of 100 objects where 20 of them are corrupt, meaning that I need 100 writing and additionally 20 data-scrubbing operations during the preservation process. With two stage hierarchical pooling an efficiency of 1.28 can be achieved. In the format-corpus dataset there were just a few file extensions of 90 that were altered and had a change rate greater than 0.0, suggesting that the two-stage strategy was not optimal, since non-volatile files would give been grouped together creating unnecessary traffic on repairing a corrupt pool. Therefore, I have proposed a context-sensitive approach where the inner group change rate of each file extension is taken into account. Each group in the ingest-bulk is assigned to an own pool with their respective pool size. For instance, in table \ref{tb:efficiency} the file extension PDF has 8 corrupted objects out of 106, which results in a pool size of 8, whereas the XML extension has a much higher corruption rate with an optimal poolside of 2. The context-sensitive approach takes these differences into account and pools homogenous groups, where groups with 0.00 change rate can be grouped with a pool size as large as the inner group. With this method, groups with low change rate can be pooled with large poolside and therefore reduce the cost and increase the efficiency even more than individual testing or two-stage hierarchical pooling.
In the third research question, where I tried to split-off metadata from the digital objects in order to create a bulk of highly volatile objects and a bulk of stable objects with low change rate, I showed that the approach is worse than with non split-off metadata. This is due to the fact, that I have to persist the double amount of digital objects and the pooling strategies could not compensate for the high amount of objects.

The limitations of the strategies proposed in this thesis is the missing data on change rates on digital objects, this is due to the fact that I would have had to monitor an archive and measure the amount of times an object has changed. In this thesis, I have arbitrarily chosen change rates for the objects and their metadata, see Table \ref{tb:split-off}. Future work may propose a method on how to estimate the change rate of an ingest in order to calculate the real optimal pool size.

The results indicate that the cost for the decentralized fixity storage on the Ethereum blockchain can be reduced, depending on the change rate of the digital objects, by at least 50\%.