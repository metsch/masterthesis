\section{Fixity Information}
Fixity Information contains the data needed to verify whether the Content Information of an object has been altered illicit or not. This data may range from verification keys, a file-location or in the case of this thesis, the transaction hash on the Ethereum blockchain \cite[8]{lee2010open}. In this thesis, fixity information will be a transaction hash, with which the fixity information can be retrieved from the Ethereum blockchain.
With fixity information you are able to answer questions regarding the authenticity and integrity of an object \cite[3]{ndsa2017fixity}.
\newline \textit{Have you received the files you expected?} With the fixity information provided with the object, you can determine that you downloaded the intended file from the data provider.
\newline \textit{Is the data corrupted or altered from what you expected?}  Besides validation that an object has not been tampered with, fixity information can be a preventive measurement against malicious software that has been attached to your file. With the fixity information provided, you are able to confirm that the file is indeed what the provider says.
\newline \textit{Can you prove you have the data/files you expected, and they are not corrupt or altered?} The data provider will strengthen the trust in its data-stewards by providing a possibility users to verify that a file is indeed correct.  \cite[2]{ndsa2014fixity}. 

For many reasons, fixity is an important concept in the long-term preservation and management of digital material. Previous research on fixity has demonstrated its critical importance in detecting changes to data and all that this error-checking can imply file authenticity and renderability, institution trustworthiness, and system monitoring/maintenance. Despite the importance of fixity in digital preservation, there is little prescriptive guidance on when and how to create fixity information, where to store it, and how frequently it should be checked. This absence is not without reason: the enormous variety of organizational structures, priorities, staffing levels, funding, resources, and collection sizes held by institutions that do digital preservation makes it difficult to establish a single set of best practices that applies to all \cite[38]{ndsa2017fixity}.
\subsection{Generating Fixity Information on Ingest}
When you take over stewardship of a piece of content, you must ensure that it is stable. Encourage content suppliers or producers to supply fixity information along with content objects whenever possible. It is critical to document fixity information as soon as possible since you can only provide assurance about the fixity of material over time once you have the fixity values. If fixity information is not provided as part of the transfer, you must create it once the materials have been received \cite[3]{ndsa2014fixity}.

\subsection{Fixity Checks}
The majority of fixity methods use a computational method that accepts a digital file as input and generates an alphanumeric value, which is used as a baseline comparison each time the fixity check is conducted \cite[5]{ndsa2017fixity}
Collections of digital files and objects should be examined on a regular basis, in addition to confirming fixity before and after transfer. There are a variety of systems and approaches for regularly testing the fixity values of all objects. This could be done on a monthly, quarterly, or annual basis, for example. The more frequently you check, the more likely you are to find and fix mistakes \cite[3]{ndsa2014fixity}.
The data must be put to good use in the form of scheduled audits of the objects against the fixity data. Replacement or repair methods must also be in place. These should, ideally, have been tested before being used. All of this is necessary for bit-level preservation, but fixity does not guarantee that the object is or will be comprehensible. Long-term access is also dependent on one's capacity to understand and utilize the file's contents in the future \cite[1]{ndsa2014fixity}.
How rapidly you can execute the checks, the complexity of your selected fixity instrument, and how much of your resources (e.g., CPU, memory, bandwidth) can be employed for this operation will all influence the rate of how often you can perform fixity checks. As the amount of digital content grows, but the infrastructure to do the checks remains the same, this could become a bottleneck. In this case, fixity checking activities may have an adverse effect on other critical services such as content delivery to consumers \cite[4]{ndsa2014fixity}.
\subsection{Fixity Instruments}
\begin{table}[p]
    \centering
    \begin{tabular}{c|p{0.4\textwidth}|p{0.4\textwidth}}
      Fixity Instrument & Definition & Level of Effort and Return of Investment\\
      \hline
      Expected File Size & File size that differs from the expected can be an indicator of problems, for example by highlighting zero byte files & Low level of effort and low level detail. File size is auto-generated technical metadata that can be viewed in Windows Explorer or other common tools. \\  
      \hline
      Expected File Count & File count that differs from the expected can be an indicator that files are either added or dropped from the package. & Low level of effort and low level detail. File count is auto-generated technical metadata that can be viewed in Windows Explorer or other common tools.  \\
     \hline
     CRC & Error detection code, typically used during network transfers. & Low level of effort and moderate level of detail. CRC function values, which are variable but typically 32 or 64 bit, are relatively easy to implement and analyze.  \\
     \hline
     MD5 & Cryptographic hash function & Moderate level of effort and high level of detail. CPU and processing requirements to compute the hash values are low to moderate depending on the size of the file. The output size of this hash value is the lowest of the cryptographic hash values at 128 bits.  \\
     \hline
     SHA1 & Cryptographic hash function & Moderate level of effort, high level of detail, and added security assurance. Due to its higher 160-bit output hash value, SHA-1 requires more relative time to compute for a given number of processing cycles CPU and processing time than MD5.  \\
     \hline
     SHA256 & More secure cryptographic hash function & High level of effort, very high level of detail, and added security assurance. With an output hash value of 256 bits, SHA-256 requires more relative time to compute for a given number of processing cycles CPU and processing time than SHA-1. 
    \end{tabular}
    \caption{Various Fixity Instruments \cite[5]{ndsa2017fixity}.}
    \label{tb:fixity-instruments}
\end{table}
The more difficult it is to discover two items that produce the same fixity information for a given fixity instrument (see Table \ref{tb:fixity-instruments}), the more "collision resistant" that instrument is. This is crucial for preventing the concealing of purposeful object alterations. Expected file size and count, for example, are particularly subject to collision: it's very easy to replace an item with one that's the same file size. For instruments with low collision resistance, it is also possible that an unintended alteration (such as corruption or human error) will result in an object with the same fixity information. The cryptographic hash functions (MD-5, SHA-1, and SHA-256) are the most collision resistant of the fixity instruments discussed above; SHA-256 is recommended for applications where security is critical. Any preservation system must, however, perform fixity checks and replace broken objects, therefore employing any fixity instrument is preferable to none at all. It is worth noting that as the hash function's level of security grows, so does the time and resources required to compute it. \cite[6]{ndsa2014fixity}.

\subsection{Storage Medium}
\label{sec:storage-medium}
There are various storages to store fixity information, each of them has some advantages and disadvantages in terms of convenience, security and throughput.
\newline \textit{In object metadata records:} In many circumstances, wherever you store and maintain metadata records, you will wish to record some file or object fixity information. These metadata records are saved as individual files or in databases. This is especially important for preserving fixity information that was initially supplied or generated as part of the long-term object metadata. 
\newline \textit{In databases and logs:} You might not want to keep adding to your object metadata records if you conduct checks at regular intervals. In this case, it is a good idea to save fixity data separately from the object in databases and logs that you may refer to later.
\newline \textit{Alongside content:} Fixity information is frequently ideal alongside the content itself. As a result, if you have issues with other layers in your system or need to migrate a group of objects, you'll still have a record of past fixity values alongside your content. For example, the BagIt specification specifies that the bagged material must be accompanied by a hash value. Similarly, some operations require the creation of *.md5 files, which are essentially text files with the md5 hash and the same name as the file they refer to, but with the.md5 extension.
\newline \textit{In the files themselves:} When a checksum is for a segment of a file, it may be more convenient to put the data in the file itself. This is solely useful for keeping sub-file fixity information within a file. Adding fixity information for a complete file to the file itself modifies the file, changing its fixity value \cite[6]{ndsa2014fixity}.
\newline \textit{Ethereum Blockchain:} In this thesis I have chosen the blockchain to be the storage-medium for the fixity information. Its immutable state and availability in addition to its novel use-case in digital archives have influenced my decision.
\section{Example of a fixity storage}
A fixity storage must persist any kind of fixity information, in my thesis SHA256 values, for long term and guarantee that the persisted content is unaltered until retrieval of the content. I have chosen the Ethereum network as a medium for persisting file fixity information, see Section \ref{sec:storage-medium} for my reasoning. In Figure \ref{fig:lifecycle} the lifecycle of fixity information is presented, where at some point in time the information is ingested into the storage and after a certain time interval the information is fetched and compared to the retrieved SHA256 value from the digital object of the archive. If both cryptographic hashes match, the object is guaranteed to be unaltered. 
\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\textwidth]{lifecycle.png}
  \caption{Example of a fixity storage}
  \label{fig:lifecycle}
\end{figure}
\section{Implementation}
\label{sec:implementation}
The fixity storage presented in this thesis is implemented in Solidity, a programming language designed for the EVM, see Section \ref{sec:evm}.
The basic functionality of the smart contract, as described in Section \ref{sec:approach} can be seen in the source code presented in Listing \ref{lst:fixity-storage}
\begin{lstlisting}[language=Solidity,caption={MVP source code of the fixity storage deployed on the Ropsten test network \url{https://Ropsten.etherscan.io/address/0x0243c7aa552730E8C6F7ED25A480a7C0c88a70f0}},label=lst:fixity-storage]
// SPDX-License-Identifier: MIT
pragma solidity >=0.4.22 <0.9.0;

contract FixityStorage {
  mapping(uint32=>bytes32) pools;
  address creator;

  constructor()public{
    creator = msg.sender;
  }

  function getPoolHash(uint32 poolId) public view returns(bytes32) {
      return pools[poolId];
  }

  function setPoolHash(uint32 poolId, bytes32 poolHash) public {
    require(msg.sender==creator);
    pools[poolId]=poolHash;
  }
}
\end{lstlisting}
where \textit{getPoolHash(uint32 poolId)} implements a read function; \textit{setPoolHash(uint32 poolId,bytes32 poolHash)} implements create and update function. The mapping type is native in Solidity which implements a hash map consisting of a key and a value, where in this case the key is an integer representing the \textit{poolId} and the value is a \textit{bytes32} object representing the SHA256 root hash of the pool. The \textit{poolId} is the reference to the local pool in the digital archive, with which fixity information can be retrieved for a certain pool from the contract. The Solidity language presents a convenient  method to prevent unauthorized calls to the setPoolHash() method, which is \textit{require(msg.sender==creator)}. The native method \textit{require} is a "guard" function which improves the readability of the smart contract code and reverts the instruction if the condition is not met. The condition in this case is, that only the creator, which is set in the constructor, of the fixity storage is able to create and alter the information stored on the blockchain.

\section{Deployment}
I utilized truffle\footnote{\url{https://trufflesuite.com/index.html}} to run my deployment of the smart contract, it brings built-in smart contract compilation, linking, deployment and binary management with automated contract testing. The reasoning behind my decision is that, truffle has all the tools needed to implement a smart contract in one package and therefore reduced complexity in the development process. In the first iteration of the development process, I used the graphical user-interface Ganache\footnote{\url{https://trufflesuite.com/ganache/index.html}} to get a better feeling for the Ethereum blockchain, see Figure \ref{fig:ganache}.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{ganache.png}
  \caption{Ganache, an interactive user interface for the Ethereum blockchain.}
  \label{fig:ganache}
\end{figure}
It allows you to navigate through your smart contract in a graphical user interface (GUI) and look at the state variables or functions to validate that your smart contract was successfully deployed. Ganache has a massive disadvantage when doing high throughput computing: it seems that it is not suited to withstand 10000 transactions built in a python for loop. Therefore, I only use the GUI it in the beginning of the experiment where I only persisted about 10 objects at a time without problems. For high throughput computing, truffle offers a command line tool to interact with the blockchain. The command line tool is resistant and shows no weakness when persisting 10,000 objects.
Truffle also allows to config other networks, e.g. the Ropsten test network, which can be defined as a parameter in the deployment process. The deployment requires to have some \acrlong{eth} tokens in your account to pay the miner to integrate your smart contract in a block. 
Truffle offers some utility regarding automated deployment, through the usage of deployment configurations where you can set the gas limit and the network configurations. Truffle offers a migrations feature, which is responsible for staging and versioning deployments. The migrations feature requires to have a smart contract deployed on the blockchain, which causes additional cost (191943 gas) to the fixity storage, therefore I have decided not to use the migrations feature, as it is controversial in the community and seen as unnecessary traffic and cost\footnote{\url{https://github.com/trufflesuite/truffle/issues/503}}.
The deployment cost of the smart contract can be calculated as follows:
\begin{equation}\label{eq:create-cost}
  \begin{split}
      C_{deployment} & = transaction + txcreate + codedeposit + txdatanonzero + txdatazero \\
      & = 21,000 + 32,000 + 200 \cdot 832 + 226 \cdot 4 + 800 \cdot 16 \\
      & = 233,104
  \end{split}
\end{equation}
where $transaction$ is the base cost for a transaction; $txcreate$ is the operation used to create a smart contract; $codedeposit$ is the gas cost for each byte of the runtime bytecode of the smart contract, which is 832 bytes and can be seen in the Input Data field of the transaction on \url{https://ropsten.etherscan.io/} with the following hash: \textit{0x844f76cfff6e00f29487f6fe3c99d8a69eab576e7c190e5b392745b48924a1f6}.
\textit{$G_{txdatanonzero}$} is 16 gas for each non-zero byte in the compiled bytecode of a transaction;\textit{$G_{txdatazero}$} is 4 gas for each non-zero byte in the compiled bytecode of a transaction; and \textit{$Contract_{bytesize}$} is the size of the compiled bytecode of the contract, see Section \ref{sec:costs} for the exact gas amount for each transaction.
The amount of gas consumed by the deployment of the decentralized fixity storage is 166,079 gas on the Ropsten testnet. The transaction used to deploy the contract can be found on \url{https://ropsten.etherscan.io/} with the following hash: \textit{0xf383c4bf0a5c32dd3369b02f68fd4e4400ef59343ad472bc96a28827f32c9abb}, where additional information, such as deployment date or the blocknumber, can be read. The address of the fixity storage, presented in Listing \ref{lst:fixity-storage} is \textit{0x0243c7aa552730E8C6F7ED25A480a7C0c88a70f0}.
\section{Authorized Access}
In Solidity one can require a certain address to access a function, the keyword \textit{requires} can be used so that a transaction from an unknown address can be reverted and only the owner of the creator address can update the mapped SHA256 values in the smart contract. An interesting topic is also how the private key in the archive may be managed, which is not part of this thesis. My suggestion for future work is to introduce a multisignature wallet which can be used to split the responsibility of the contract owning Ethereum account, the creator account. Since the entity which controls the private key of the creator account is able to create and update fixity information on the blockchain. In the case of a compromised creator account, each entry of fixity information may be invalidated by the archive and new fixity information have to be uploaded to the blockchain. The key user management of the creators address may be investigated in further research. For this thesis I assume that the private key of the creator account is well managed and each transaction coming from the creator's address is legit.
\section{Cost of interacting with the fixity storage}\label{sec:cost-interating}
The cost of interacting with the fixity storage depends on the desired action. There are three different ways to interact with the contract: (1) create a new entry (2) update an existing entry and (3) read an entry.
The cost of a transaction, which invokes the \textit{setPoolHash} function and ultimately stores 256 bit on the blockchain can be calculated with Equation \ref{eq:tx-cost}
\begin{equation}\label{eq:tx-cost}
    \begin{split}
        C_{setPoolHash} & = gasAmount \cdot gasPrice \cdot ethPrice \\ 
        & = 42,368 \cdot 0.00000005 \cdot 4,000
    \end{split}
\end{equation}
which results in \$8.47 on average for an ETH price of \$4000.
The $gasAmount$ in Equation \ref{eq:tx-cost} can be calculated as follows:
\begin{equation}\label{eq:tx-data}
  \begin{split}
    gasAmount & = transaction + sset + txdatazero + txdatanonzero \\
     & = 21,000 + 20,000 + 16 \cdot 68 + 4 \cdot 70  \\
     & = 42,368
  \end{split}
\end{equation}
where $txdatazero$ is the number of zeros in the transaction data and $txdatanonzero$ is the amount of non-zero bytes in the transaction data. The transaction data for Equation \ref{eq:tx-data} can be found in the \textit{Input Data} field of the transaction on Etherscan\footnote{\url{https://ropsten.etherscan.io/tx/0x8839e03f0143bad34060fa909c35d30f2edb22dd4fdac0264de8ae84176eb1ea}}. This field contains the binary data that formed the input to the transaction, the structure is depending on which type of transaction is executed. Whether the transaction is contract call, contract deployment or message call the input data holds instructions in form of bytecodes to properly execute the desired interaction. The hexadecimal form of the input data is used for the calculation of $txdatazero$ and $txdatanonzero$.

The cost for updating an existing entry can be calculated in the same way as for creating ones, with the difference that the costly $sset$ operation can be spared and therefore updates costs 20,000 gas less than creates.
Reading an entry is free of charge, since no state changes has to be forwarded to the blockchain.
To explain the cost of the non-zero and zero bytes can be explained by looking the following \textit{Input Data} as a hexadecimal value:

\hash{0x178d292900000000000000000000000000000000000000000000000000000000000000001760d27083f6e2d1c46a65938c03a0c52dccf55cb4eb68a720e6efe3a8851f78}

where zero and nonzero bytes are counted and multiplied by their respective gas cost, which in this example is $txdatazero+txdatanonzero = 4 \cdot 70 + 16 \cdot 68 = 1368 $. The amount of non-zero bytes can be interpreted as the amount of effort given to the network.

\section{Proof of Concept}\label{sec:poc}
The proof of concept is written in Python in form of a Jupyter Notebook. It can be found on GitHub\footnote{\url{https://github.com/metsch/masterthesis/blob/main/src/py/poc_ropsten.ipynb}}. With Python's Web3.py I get the contract from the blockchain and extract its functions \textit{setPoolHash} and \textit{getPoolHash}. For the live experiment on the Ropsten test network I have initialized 100 digital objects, in form of SHA256 values, with a change rate of 0.1, meaning that \%10 of the ingested objects will be changed during the storage in the archive. The low amount of objects in the live experiment is due to the fact, that the Ropsten \acrlong{eth} token is hard to get, since the most relevant publicly available faucets are drained out and the ones available have a rate of only 0.1 \acrlong{eth} per day, as I explained in Section \ref{sec:test-nets}. I compute the optimal pool size k for N=100 and p=0.1 with Equation \ref{eq:poolsize} and created $\lceil N/k \rceil$ pools. Each pool is assigned a unique identifier and holds an array of objects, from which a hash list is formed, and the resulting root hash is later persisted on the blockchain. 
Before persisting the root hashes, I estimate the gas cost for a single transaction in order to set the gas limit, a transaction will revert if the cost exceeds the manually set gas limit. I utilized Equation \ref{eq:tx-data} and added 20\% in order guarantee that the transaction is not underpaid, which may happen if the network is congested at the time of firing the transaction.
At the time of the experiment, the gas price was 86.11 gwei which is 0,0000000896\acrshort{eth}, resulting in \$15.18 for a \textit{setPoolHash} transaction calculated with Equation \ref{eq:tx-cost} with an \acrshort{eth} price of \$4000. Admitted, this gas price is really high, usually the gas price is about 40 gwei. 
For each pool, 25 in total, I uploaded each root hash on to the blockchain and waited for the transaction to finish and run some tests to see if the transactions were successful.
After uploading the root hashes, I artificially corrupted the objects in the archive with a Bernoulli trial where an object gets corrupted if a random number between 0 and 1 is below p=0.1. At last, I repaired the archive by checking the local pools with the ones on the blockchain, and if the pool hashes did match the pool is seen as uncorrupted. Where the pools with non-matching hashes got replaced by copies of the objects.
To sum it up, the cost for the live experiment was for 25 writing transactions 0.11204969029 ETH (\$448) and 8 data-scrubbing operations resulting in 33 total operations. Data-scrubbing is an operation, where a corrupted object in the archive gets replaced with a fresh copy. The copy is assumed to be uncorrupted with the original hash value, therefore no further writing transaction is needed when you replace an object in the archive. Whereas, with an individual testing strategy the amount of writing transactions would have been 100 and estimated 10 data-scrubbing operations.

\section{Summary}
In this chapter I have presented the various forms of fixity information with their respective advantages and disadvantages together with my reasoning on why I have decided to utilize SHA256 cryptographic hashes. I have also presented various storage-media for fixity information and why it is so important that these storages guarantee for immutability in regard to history forgery. The interactions with the fixity storage are tested in an experimental environment, presented in the preceding section \ref{sec:poc}.