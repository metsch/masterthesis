\section{Setup}
The experiment written in Python in form of a Jupyter Notebook. At first the function for providing the optimal poolsize was implemented, the exact implementation can range froma bernoulli experiment to another form. When the optimal pool sie is found, which is the econd research question I create 10000 objects and assign them to a certain pool and assign them a corruption rate ranging from 0.01 to 0.2. The corruption rate is the base for the bernoulli experiment. After object creation, the objects are ingested into the archive, durng this process each pool is persisted on the blockchain. This happens in an interaction between a python client and a local installation of the ethereum blockchain. Ganache is used for local development on the Ethereum side and pythons Web3 is used as a client. While ingesting and uploading onto the blockchain i will monitor the amount of gas used and the exact number of write transactions. The more eact method of monitoring the operation cost is by adding up the gas cost, since gas can be transformed nto ETH. The first measurement will be theoretical and the second empirical where i monitor the amount of ETH available for the experiment account. e.g. if i start the experiment with 100 ETH and at the end i have 90 ETH left the operation cost was 10 ETH. If the experiment local is successfulll I willl deploy the smart contract on the ropsten testnet and add time measurement to the porcess since the blockchain is now real and distributed on the network. I expect the time needed to ingest and upload on the blockchain to increase the cost should stay the same. When the objects are ingested into thedummy archive and the pools are uploaded onto the blockchain i will retrieve random samples from the original set of objects and check if they are corrupt, maybe i will assign corruption at random to this random sample and then I have to implement a repair function, this ffunction also affect the operation cost, since when a corrupted file is found the whole pool has to be re-computed resulting in poolsize + 1 local transactions and one blockchain write transaction. In the retrieval process i will monitor the time and nullify the cost since read operations on the blockchain are free of charge, nonetheless i will count the number of transactions needed for the process. After retrieval and repairing the pools i will look into the number of transactions needed and the time needed for the whole process. The result will be one row af a table and the experiment can be redone with different poolsies or corruption rates to get a nice table with descriptive statistics. 
At last I will split the objects and metadata and adapt the corruptions rates and redo the whole experiment and compare the results. 
\section{Object Identity}
when and where should the pool id be stored, there are possibilities that the pool id is assigned with pool creation but that would mean that i have to update the object which is supposed to preserve, should the pool id be in the metadata of the object= if no, where should we store the link between the oject and its respective pool. Its easy to keep the links in the local jupyter notebook but in a industrial environment keeiping those links is rather hard. For now i keep the transaction hash with the pool object
\section{Object}
An object in this experiment is simply a sha256 hash, since the preservation process does not care if a picture, text or video is secured by the hash. An object also holds the reference to its pool, where in a real case the pool_id is stored in the object's metadata. For the sake of the experiment, the initial bulk is numbered from 1 to N, where the sha256(n) is then assigned to the object. An object also has a float value which indicates the chance of it of being corrupted, this value is used in the bernoulli experiment to determine the optimal pool size based on the corruption_rate of all objects. The final member variable of the object is a boolean flag which indicates wheter an object is corrupted or not.

\section{Pool}
A pool in this experiment is a collection of objects with size k. The root hash of a pool is a hash-list of every object in the pool.
\section{Archive Mock}
For the sake of the experiment I will mock the functions of a digital archive in python with the following relevant functions implemented: (1) bulk_ingest (2) retrieve (3) get_objects_by_pool_id (4) repair. (1) Is for ingesting a bulk of objects with their respective metadata, it is expected that the pool_id is already set for the object. The bulk itself does not know anything about the pool sizes or the amount of pools. The archive therefore is independent of the implementation of a pool and only need to know the pool_id of an object. The pool_id may be stored in the Preservation Description Information of an object \cite{lee2010open}. (2) Retrieve a single object from the archive. (3) The implementation of this function in a real digital archive may vary from my implementation, where I return every objects stored in the archive with matching pool_id. This function is here to provide information regarding the original objects of a pool, this is necessary to rebuild the pool and recalculate the pool hash for someone who might want to check wheter an object in the archive got corrupted. (4) The function which handles the case when a corrupted pool was found. It is a costly function where one write transaction to the blockchain and additional data scrubbing has to be made. When a pool is found to be corrupted, each object in the pool is suspected to be corrupted too and therefore each object has to be replaced by a copy and reassembled in a pool, which hash is then again persisted on the blockchain.
The mock also provides a function to simulate corruption, where each object in the archive has a chance to corrupt itself.
\section{Program Flow}
During development I have seen that the pool size should not be too high.
When a corrupted pool is found, make sure to not double write the same pool to the blockchain
You even got an advantage when you repair objects, since you have do scrub each object in the pool with a fresh copy.
\section{Findings}
in the cleaning process i cant count the reall number of corrupt objects because the number gets diluted by the fact that the local pools are false and even when we get a real object we could not recalculate the pool because the pool itself is corrupted therefore an uncorrupted object is seen here as corrupted since the pool cannot prove it anymore
a positive sideffect is if you find a corrupt elemen you have to scrub the whole pool, which means that the whole pool has is replaced with fresh copies
There are massivele more repairing transactions

