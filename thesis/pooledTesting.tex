\section{Pooled Testing}
Pooled Testing was first introduced by \cite{dorfman1943detection} as a strategy to screen a large number of military recruits for syphiclis during World War 2. Dorfman envisioned that instead of testing each recruit's blood specimen separately, multiple specimens could be pooled together and tested at once. Individuals from negative pools would be declared negative, and specimens from positive pools would be retested individually to identify which recruits had contracted the disease. Dorfmanss motivation for using group testing was to reduce testing costs while still identifying all syphilitic-positive recruits. Today, this would be described as the “case identification problem,” because the goal is to identify all positive individuals among all individuals tested. Dorfmanss approach to case identification can be viewed as a two-stage hierarchical algorithm. In this approach, non-overlapping pools are tested in the first stage and individuals from positive pools are tested in the second stage. When the corruption prevalence is small, higher-stage algorithms have proven to be useful at further reducing the number of tests needed \cite[1]{hou2017hierarchical}.
\section{Hierarchical pooling algorithms}
Hierarchical algorithms involve testing samples in non-overlapping pools over a pre-defined number of stages, until each individual can be classified as positive or negative \cite[2]{lagopati2021sample}.
\section{Two Stage Hierarchical Pooling Algorithm} \label{two-stage-hierarchical}
In the first stage of this protocol N pools get initialized and filled with samples of the population. If the combined result of the pool is negative than no second stage is needied, but when a pool is declared positive a second stage is needed and all individuals from this pool have to be retested in order to find the corrupted individual. The expected number of test is equal to the number of tests in the first stage added to the number of tests in the second stage \cite[3]{nianogo2021optimal}. The optimal pool size is the size that minimizes the total number of tests needed.
\section{Parameter Definition}
\begin{table}[t]
    \centering
    \caption{Various Fixity Instruments \cite[6]{ndsa2017fixity}}
    \label{tb:fixity-instruments}
    \begin{tabular}{ c c }
        Parameter     &  description \\
        p     &  prevalence of corruption, the probability that an object will be corrupted during the preservation process \\
        1-p   &  the probability that the integrity of an object is preserved during the preservation process \\   
        $(1-p)^n$ &  the probability of obtaining a uncorrupted result from a pool of $n$ objects \\  
        $1 - (1-p)^n$ &  the probability of obtaining a corrupted result from a pool of $n$ objects \\  
        N/j & the number of pools of size j in a population of size N \\  
        p'N/j & the expected number of corupted pools of n in a poluation of N with a corruption rate of p \\  
        $E(T) = N/j + n(N/n)p'$ & the expected number of tests 
    \end{tabular}
\end{table}
\cite[3]{dorfman1943detection}
\section{Non-Hierarchical pooling algorithms}
Non-hierarchical algorithms involve testing over stages. The difference is that in this approach, individuals may be tested more than once per stage through overlapping pools \cite[3]{lagopati2021sample}
\section{Optimal poolsize}
There are a few methods to determine the optimal poolsize. The most that I have found during literature review did not consider cost in the retesting stage, meaning that the poolsize could potentionally be N, therefore you get 2 total transactions to do the process, one for ingest and one for the repairing. But then you would have to update your whole archive each time an object is found corrupted, since the pool securing the object is the whole archive. In my experiment, I have to find a way to add a cost to the optimal poolsize to prevent too large pool sizes.
The optimal pool size to minimize write transactions on the blockchain is N, because if we can secure every object in the archive with only one write transaction. That would be very inpracticable in a real environment, because you would have to compute a new root hash on each update for each pool. Therefore shooting up the actions with every update for N actions.
 